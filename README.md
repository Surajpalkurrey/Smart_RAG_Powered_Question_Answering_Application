# Smart_RAG_Powered_Question_Answering_Application.
Overview
This project implements a Retrieval-Augmented Generation (RAG) based application for answering user queries by processing different types of inputs, including PDF files, URLs, and web search results. The app uses a locally hosted Ollama Large Language Model (LLM) to generate context-aware answers, providing fast and reliable information retrieval. The app is built using Python, Streamlit for the frontend, and integrates Chroma for embedding-based document retrieval.

Key Features
Multi-input Support: Accepts inputs from PDFs, URLs, or web search queries.
Locally Hosted LLM: Powered by Ollama's locally run LLM for faster and more secure query processing.
Embeddings-based Retrieval: Utilizes Chroma to store document embeddings for efficient context retrieval and answer generation.
User-Friendly Interface: Streamlit-based interface for seamless user experience and dynamic progress tracking during query processing.
